---
tags:
  - type/permanent
  - attr/technique
  - cs/llm
  - ml/fine-tuning
---

# SFT - 监督式微调 (Supervised Fine-Tuning)

## 定义
用有标签的 (instruction, response) 对数据集训练 LLM，使其学习特定任务行为和输出格式。

## 核心特征
- **参数更新**：100% 的模型权重都参与梯度更新
- **性能天花板**：最高（无参数约束）
- **计算成本**：高（显存和时间成本大）
- **灾难性遗忘**：风险较大

## 关键参数
- `learning_rate`: 2e-5 ~ 5e-5（比预训练小得多）
- `batch_size`: 8 ~ 32（取决于显存）
- `epochs`: 2 ~ 5（防止过拟合）
- `warmup_ratio`: 0.1 ~ 0.2

## 流程
1. 准备高质量 (instruction, response) 对数据集
2. 设置标准交叉熵损失函数
3. 在微调数据上迭代更新权重
4. 在验证集上测试任务相关指标

## 适用场景
- 数据充足（10k+ 高质量样本）
- 计算资源充富（8x80GB GPU）
- 任务特异性强

## 与其他方法的关系
- [[PEFT]] 的基础方法（无约束）
- [[LoRA]] 和 [[OFT]] 的对标方案
- [[DPO]] 和 [[KTO]] 的前置阶段

## 注记
在实践中，SFT 通常用作其他微调方法的初始化阶段。单独全量微调成本高，常结合 [[参数冻结策略]] 使用。
