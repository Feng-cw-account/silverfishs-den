---
tags:
  - type/permanent
  - attr/technique
  - cs/llm
  - ml/fine-tuning
---

# 参数冻结策略 (Freezing Strategy)

## 定义
在微调时，选择性冻结（不更新）模型的某些层，仅训练其他层。是一种常见的 [[PEFT|参数高效微调]] 手段。

## 核心策略

### 冻结底层（保护通用特征）
- **何时**：预训练知识很重要，微调数据有限
- **做法**：冻结 embedding + 前 6-12 层
- **原因**：底层捕捉通用特征，防止灾难性遗忘

### 仅微调顶层（快速任务适配）
- **何时**：任务特异性强，数据充足
- **做法**：仅微调最后 3-6 层 + head
- **原因**：快速适配，参数少，风险小

### 结合 PEFT 方法（中等冻结）
- **何时**：使用 [[LoRA]] 或 [[OFT]]
- **做法**：冻结大部分权重，仅在 Q/V/K/O 层添加 LoRA
- **原因**：平衡适配能力与稳定性

### 少样本学习（极度冻结）
- **何时**：数据极少（Few-shot）
- **做法**：仅微调 head 和最后 1-2 层
- **原因**：极少数据无法安全训练全部权重

## 实践建议
- 使用 `requires_grad = False` 冻结特定参数
- 监控冻结层和微调层的损失变化
- 可在微调中途动态解冻层（例如先冻结再逐步解冻）
- 与 [[SFT]]、[[LoRA]]、[[OFT]] 灵活组合

## 与其他方法的关系
- 与 [[PEFT]] 方法正交（两者可同时使用）
- 与 [[SFT]] 配合可避免灾难性遗忘
- 与 [[DPO]] 和 [[KTO]] 阶段常结合使用

## 注记
参数冻结不一定意味着更好的结果，需根据数据量、任务差异度和计算资源进行权衡。
