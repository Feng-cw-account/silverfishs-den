---
tags:
  - type/permanent
  - attr/technique
  - cs/llm
  - ml/rl-alignment
---

# PPO - 近端策略优化 (Proximal Policy Optimization)

## 定义
一种强化学习算法，通过奖励模型指导 LLM 生成更符合人类偏好的输出。需要先用 [[SFT]] 训练基础模型。

## 核心流程
1. **有监督微调** (SFT)：在 (prompt, response) 数据上训练基础模型
2. **奖励建模**：收集人类标注的 response 偏好对，训练奖励模型 $R(x, y)$
3. **PPO 训练**：
   - 策略模型生成文本
   - 奖励模型评分
   - 计算 PPO 损失，更新策略模型参数
   - 保持与 SFT 模型的 KL 散度约束

## 关键优点
- 显式优化人类偏好，效果可验证
- PPO 算法稳定、收敛快

## 主要缺点
- 需要大规模人工标注数据（通常 10k+）
- 计算成本高（需同时运行策略模型、奖励模型、价值模型）
- 奖励模型本身可能有偏差

## 与 [[DPO]] 和 [[KTO]] 的对比

| 特性 | PPO | DPO | KTO |
|------|-----|-----|-----|
| 需要奖励模型 | 是 | 否 | 否 |
| 训练步骤数 | 多 | 少 | 少 |
| 计算资源 | 高 | 中等 | 中等 |
| 收敛速度 | 较慢 | 更快 | 更快 |

## 应用场景
- 有充足人工标注数据
- 对齐效果要求极高
- 计算资源充富

## 注记
PPO 是目前工业界最常用的对齐方法（如 ChatGPT、Claude），但成本高。近年 [[DPO]] 和 [[KTO]] 等无奖励模型方法逐渐流行。
