---
tags:
  - type/permanent
  - attr/concept
  - cs/llm
  - ml/alignment
---

# 灾难性遗忘 (Catastrophic Forgetting)

## 定义
在微调预训练 LLM 时，新数据的训练会导致模型遗忘原始预训练任务的知识，导致在通用任务上性能下降。

## 现象
- **原症状**：模型在新任务上精度提升，但在原始任务或通用语言理解上精度下降
- **根本原因**：反向传播的梯度更新权重时，改变了预训练时形成的特征表示

## 防止策略

### 1. 使用 PEFT 方法
[[LoRA]] 和 [[OFT]] 通过冻结原权重、仅在低秩空间微调，天然防止灾难性遗忘。

### 2. 参数冻结
[[参数冻结策略]] - 冻结底层保留通用特征，仅微调上层。

### 3. 混合数据训练
在微调数据中混入部分通用数据（如原预训练数据的子集），维持模型在通用任务上的能力。

### 4. 低学习率与正则化
- 使用较小的学习率（2e-5 ~ 5e-5）
- 添加权重衰减或 L2 正则化
- 使用早停防止过度微调

### 5. KL 散度约束
在 [[PPO]] 和偏好优化（[[DPO]]、[[KTO]]）中，约束新模型与参考模型的 KL 散度，防止偏离太远。

## 衡量指标
- **在微调数据上的精度**：new task accuracy
- **在原始任务上的精度**：general task accuracy（如 MMLU、HellaSwag）
- **综合指标**：加权平均或 Pareto frontier

## 与其他概念的关系
- [[PEFT]] 的设计初衷之一就是防止灾难性遗忘
- [[参数冻结策略]] 的直接应用
- [[DPO]] 和 [[KTO]] 中都有 KL 约束来限制灾难性遗忘

## 注记
灾难性遗忘是微调中的核心挑战。模型的通用能力保护与任务适应的平衡，是高效微调的关键。
